Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Palaz2015,
abstract = {Automatic speech recognition systems typically model the rela-tionship between the acoustic speech signal and the phones in two separate steps: feature extraction and classifier training. In our recent works, we have shown that, in the framework of con-volutional neural networks (CNN), the relationship between the raw speech signal and the phones can be directly modeled and ASR systems competitive to standard approach can be built. In this paper, we first analyze and show that, between the first two convolutional layers, the CNN learns (in parts) and models the phone-specific spectral envelope information of 2-4 ms speech. Given that we show that the CNN-based approach yields ASR trends similar to standard short-term spectral based ASR sys-tem under mismatched (noisy) conditions, with the CNN-based approach being more robust.},
author = {Palaz, Dimitri and Magimai-Doss, Mathew and Collobert, Ronan},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
issn = {19909772},
keywords = {Automatic speech recognition,Convolutional neural networks,Raw signal,Robust speech recognition},
title = {{Analysis of CNN-based speech recognition system using raw speech as input}},
year = {2015}
}
@article{Jozefowicz2015,
abstract = {The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
doi = {10.1109/CVPR.2015.7298761},
eprint = {1512.03385},
isbn = {9781937284978},
issn = {1045-9227},
journal = {JMLR},
keywords = {deep learning,denoising auto-encoder,image denoising},
pmid = {18267787},
title = {{An Empirical Exploration of Recurrent Network Architectures}},
year = {2015}
}
@article{Chen1997,
abstract = {We describe new methods for speaker-independent, continuous mandarin speech recognition based on the IBM HMM-based continuous speech recognition system (1-3): First, we treat tones in mandarin as attributes of certain phonemes, instead of syllables. Second, instantaneous pitch is treated as a variable in the acoustic feature vector, in the same way as cepstra or energy. Third, by designing a set of word-segmentation rules to convert the continuous Chinese text into segmented text, an effective trigram language model is trained(4). By applying those new methods, a speaker-independent, very-large-vocabulary continuous mandarin dictation system is demonstrated. Decoding results showed that its performance is similar to the best results for US English.},
author = {Chen, C Julian and Gopinath, Ramesh A and Monkowski, Michael D and Picheny, Michael A and Shen, Katherine},
journal = {5th European Conference on Speech Communication and Technology (EUROSPEECH)},
title = {{New methods in continuous Mandarin speech recognition.}},
year = {1997}
}
@article{Taguchi,
abstract = {Methods for synthesizing speech sounds from the motion of ar-ticulatory organs can be used to produce substitute speech for people who have undergone laryngectomy. To achieve this goal, feature parameters representing the spectral envelope of speech, directly related to the acoustic characteristics of the vocal tract, has been estimated from articulatory movements. Within this framework, speech can be synthesized by driving the filter obtained from a spectral envelope with noise signals. In the current study, we examined an alternative method that generates speech sounds directly from the motion pattern of articulatory organs based on the implicit relationships between articulatory movements and the source signal of speech. These implicit relationships were estimated by considering that articulatory movements are involved in phonological representations of speech that are also related to sound source information such as the temporal pattern of pitch and voiced/unvoiced flag. We developed a method for simultaneously estimating the spectral envelope and sound source parameters from articulatory data obtained with an electromagnetic articulography (EMA) sensor. Furthermore, objective evaluation of estimated speech parameters and subjective evaluation of the word error rate were performed to examine the effectiveness of our method.},
author = {Taguchi, Fumiaki and Kaburagi, Tokihiko},
doi = {10.21437/Interspeech.2018-999},
file = {:home/booomkin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taguchi, Kaburagi - Unknown - Articulatory-to-speech conversion using bi-directional long short-term memory(2).pdf:pdf},
keywords = {Articulatory-to-acoustic mapping,Deep learning,EMA,Index Terms: Articulatory movement,Vocal-tract spec-trum},
title = {{Articulatory-to-speech conversion using bi-directional long short-term memory}},
url = {https://www.isca-speech.org/archive/Interspeech{\_}2018/pdfs/0999.pdf}
}
@article{Gonzalez2016,
abstract = {In this paper we present a silent speech interface (SSI) system aimed at restoring speech communication for individuals who have lost their voice due to laryngectomy or diseases affecting the vocal folds. In the proposed system, articulatory data captured from the lips and tongue using permanent magnet articulography (PMA) are converted into audible speech using a speaker-dependent transformation learned from simultaneous recordings of PMA and audio signals acquired before laryngectomy. The transformation is represented using a mixture of factor analysers, which is a generative model that allows us to efficiently model non-linear behaviour and perform dimensionality reduction at the same time. The learned transformation is then deployed during normal usage of the SSI to restore the acoustic speech signal associated with the captured PMA data. The proposed system is evaluated using objective quality measures and listening tests on two databases containing PMA and audio recordings for normal speakers. Results show that it is possible to reconstruct speech from articulator movements captured by an unobtrusive technique without an intermediate recognition step. The SSI is capable of producing speech of sufficient intelligibility and naturalness that the speaker is clearly identifiable, but problems remain in scaling up the process to function consistently for phonetically rich vocabularies.},
author = {Gonzalez, Jose A. and Cheah, Lam A. and Gilbert, James M. and Bai, Jie and Ell, Stephen R. and Green, Phil D. and Moore, Roger K.},
doi = {10.1016/j.csl.2016.02.002},
isbn = {978-989-758-170-0},
issn = {10958363},
journal = {Computer Speech and Language},
keywords = {Augmentative and alternative communication,Permanent magnet articulography,Silent speech interfaces,Speech rehabilitation,Speech synthesis},
pmid = {9554183},
title = {{A silent speech system based on permanent magnet articulography and direct synthesis}},
year = {2016}
}
@misc{Halpern2019,
author = {Halpern, Bence},
title = {{Articulatory vocoder}},
url = {http://github.com/karkirowle/vocoder-clean}
}
@article{Wu2016,
abstract = {We propose two novel techniques --- stacking bottleneck features and minimum generation error training criterion --- to improve the performance of deep neural network (DNN)-based speech synthesis. The techniques address the related issues of frame-by-frame independence and ignorance of the relationship between static and dynamic features, within current typical DNN-based synthesis frameworks. Stacking bottleneck features, which are an acoustically--informed linguistic representation, provides an efficient way to include more detailed linguistic context at the input. The minimum generation error training criterion minimises overall output trajectory error across an utterance, rather than minimising the error per frame independently, and thus takes into account the interaction between static and dynamic features. The two techniques can be easily combined to further improve performance. We present both objective and subjective results that demonstrate the effectiveness of the proposed techniques. The subjective results show that combining the two techniques leads to significantly more natural synthetic speech than from conventional DNN or long short-term memory (LSTM) recurrent neural network (RNN) systems.},
archivePrefix = {arXiv},
arxivId = {1602.06727},
author = {Wu, Zhizheng and King, Simon},
doi = {10.1109/TASLP.2016.2551865},
eprint = {1602.06727},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Minimum generation error,Speech synthesis,acoustic modelling,bottleneck,deep neural network},
title = {{Improving Trajectory Modelling for DNN-Based Speech Synthesis by Using Stacked Bottleneck Features and Minimum Generation Error Training}},
year = {2016}
}
@article{pysptk,
title = {{PySPTK toolkit}},
url = {http://github.com/r9y9/pysptk}
}
@inproceedings{Selvaraju2017,
abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any architectural changes or re-training. We combine GradCAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on weakly-supervised localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we conduct human studies to measure if GradCAM explanations help users establish trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a "stronger" deep network from a "weaker" one. Our code is available at https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.},
archivePrefix = {arXiv},
arxivId = {1610.02391},
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.74},
eprint = {1610.02391},
isbn = {9781538610329},
issn = {15505499},
pmid = {24880761},
title = {{Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization}},
year = {2017}
}
@inproceedings{Richmond2011,
abstract = {This paper serves as an initial announcement of the avail- ability of a corpus of articulatory data called mngu0. This cor- pus will ultimately consist of a collection of multiple sources of articulatory data acquired from a single speaker: electro- magnetic articulography (EMA), audio, video, volumetric MRI scans, and 3D scans of dental impressions. This data will be provided free for research use. In this first stage of the release, we are making available one subset of EMA data, consisting of more than 1,300 phonetically diverse utterances recorded with a Carstens AG500 electromagnetic articulograph. Distribution of mngu0 will be managed by a dedicated “forum-style” web site. This paper both outlines the general goals motivating the distribution of the data and the creation of the mngu0 web fo- rum, and also provides a description of the EMA data contained in this initial release.},
author = {Richmond, Korin and Hoole, Phil and King, Simon},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
isbn = {19909772 (ISSN)},
issn = {19909772},
keywords = {Articulography,Corpus,EMA},
title = {{Announcing the electromagnetic articulography (day 1) subset of the mngu0 articulatory corpus}},
year = {2011}
}
@article{Aryal2016,
abstract = {The conventional approach for data-driven articulatory synthesis consists of modeling the joint acoustic-articulatory distribution with a Gaussian mixture model (GMM), followed by a post-processing step that optimizes the resulting acoustic trajectories. This final step can significantly improve the accuracy of the GMM frame-by-frame mapping but is computationally intensive and requires that the entire utterance be synthesized beforehand, making it unsuited for real-time synthesis. To address this issue, we present a deep neural network (DNN) articulatory synthesizer that uses a tapped-delay input line, allowing the model to capture context information in the articulatory trajectory without the need for post-processing. We characterize the DNN as a function of the context size and number of hidden layers, and compare it against two GMM articulatory synthesizers, a baseline model that performs a simple frame-by-frame mapping, and a second model that also performs trajectory optimization. Our results show that a DNN with a 60-ms context window and two 512-neuron hidden layers can synthesize speech at four times the frame rate - comparable to frame-by-frame mappings, while improving the accuracy of trajectory optimization (a 9.8{\%} reduction in Mel Cepstral distortion). Subjective evaluation through pairwise listening tests also shows a strong preference toward the DNN articulatory synthesizer when compared to GMM trajectory optimization.},
author = {Aryal, Sandesh and Gutierrez-Osuna, Ricardo},
doi = {10.1016/j.csl.2015.02.003},
file = {:media/booomkin/Windows/Users/b.halpern/Downloads/aryal2016csl.pdf:pdf},
issn = {10958363},
journal = {Computer Speech and Language},
keywords = {Articulatory synthesis,Deep learning,Electromagnetic articulography,Gaussian mixture models},
pages = {260--273},
publisher = {Elsevier Ltd},
title = {{Data driven articulatory synthesis with deep neural networks}},
url = {http://dx.doi.org/10.1016/j.csl.2015.02.003},
volume = {36},
year = {2016}
}
@article{Liu2018,
abstract = {This paper presents a method to convert articulatory movements into speech waveforms using a data-driven approach. In this method, recorded electromagnetic midsagittal articulography (EMA) measurements are converted into both spectral features (i.e., Mel-cepstra) and excitation features (i.e., power, voiced/unvoiced flag, and F0) from which speech waveforms are then reconstructed. By considering the nonlinear and dynamic dependency relationships between articulatory movements and acoustic signals, this study adopts bidirectional long short-term memory (BLSTM) based recurrent neural networks (RNN) for the articulatory-to-acoustic conversion. Due to the limitations of current data acquisition technology, recorded articulatory movements are inevitably insufficient to completely describe the articulatory configuration during pronunciation. Therefore, this paper proposes to further augment the model input by concatenating EMA vectors with two other representations. First, the posterior probabilities derived from a phoneme classifier are concatenated with EMA features to provide a linguistic description of each frame for acoustic feature prediction. The classifier is trained to determine the phoneme label of each frame based on the observed EMA features. Second, a cascaded prediction strategy is designed to utilize the predicted spectral features as auxiliary input to boost the prediction accuracy of the excitation features. The results of experiment show that BLSTM-RNNs can achieve a better objective and subjective performance than deep neural networks (DNN) and Gaussian mixture models (GMM) in articulatory-to-acoustic conversion. In addition, our results show that the proposed methods for integrating linguistic representation and utilizing a cascaded prediction strategy can further improve the accuracy of acoustic feature prediction.},
author = {Liu, Zheng Chen and Ling, Zhen Hua and Dai, Li Rong},
doi = {10.1016/j.specom.2018.02.008},
file = {:media/booomkin/Windows/Users/b.halpern/Downloads/1-s2.0-S0167639317303138-main.pdf:pdf},
issn = {01676393},
journal = {Speech Communication},
keywords = {Articulatory-to-acoustic conversion,Deep neural network,Gaussian mixture model,Long short-term memory,Recurrent neural network},
number = {February},
pages = {161--172},
publisher = {Elsevier},
title = {{Articulatory-to-acoustic conversion using BLSTM-RNNs with augmented input representation}},
url = {https://doi.org/10.1016/j.specom.2018.02.008},
volume = {99},
year = {2018}
}
@misc{Speech2019,
author = {Halpern, Bence},
title = {{Speech Examples}},
url = {http://karkirowle.github.io/paper1}
}
@book{Benesty2009,
abstract = {This handbook is meant to play a fundamental role for sustainable progress in speech research and development.},
author = {Benesty, Jacob and Sondhi, Mohan M. and Huang, Yiteng and Greenberg, Steven},
booktitle = {The Journal of the Acoustical Society of America},
doi = {10.1121/1.3203918},
file = {:home/booomkin/Documents/Jacob Benesty Prof., M. Mohan Sondhi Ph.D, Yiteng (Arden) Huang Dr. (auth.), Prof. Jacob Benesty Dr., Prof. M. Mohan Sondhi Ph.D., Prof. Yiteng Arden Huang Dr. (eds.)-Springer Handbook of Speech Proce (1).pdf:pdf},
isbn = {9783540491255},
issn = {00014966},
number = {4},
pages = {2130},
pmid = {19813827},
title = {{Springer Handbook of Speech Processing}},
url = {http://scitation.aip.org/content/asa/journal/jasa/126/4/10.1121/1.3203918},
volume = {126},
year = {2009}
}
@article{Morise2016,
abstract = {A vocoder-based speech synthesis system, named WORLD, wasdevelopedinaneffort to improve the sound quality of real- time applications using speech. Speech analysis, manipulation, and synthe- sis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been devel- oped, real-time processing has been difficult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis al- gorithms and one synthesis algorithm proposed in our previous research. The effectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.},
author = {Morise, Masanori and Yokomori, Fumiya and Ozawa, Kenji},
doi = {10.1587/transinf.2015EDP7457},
issn = {17451361},
journal = {IEICE Transactions on Information and Systems},
keywords = {Realtime processing,Sound quality,Speech analysis,Speech synthesis,Vocoder},
title = {{WORLD: A vocoder-based high-quality speech synthesis system for real-time applications}},
year = {2016}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
isbn = {08997667 (ISSN)},
issn = {08997667},
journal = {Neural Computation},
pmid = {9377276},
title = {{Long Short-Term Memory}},
year = {1997}
}
@article{Fant1981,
abstract = {This is a review of recent work on source-filter composition of voiced sounds, describing various alternatives of defining a source function with implications for a more profound understanding of the acoustic processes involved and their modelling in equivalent networks. The theoretical study is backed up by experiments with inverse filtering and other types of glottography. Special attention is devoted to time-varying bandwidths within a glottal open period and the implications for defining overall effective bandwidths. The presence in the speech wave of voice source components as well as the presence of formant oscillation in the glottal flow are discussed. Expressions for supra- and sub-glottal pressure fluctuations are also derived. An earlier proposed parametric model of glottal flow and formant excitation is reviewed and discussed with respect to voice intensity and some dynamic factor in connected speech.},
author = {Fant, Gunnar},
issn = {11045787},
url = {http://www.speech.kth.se/gpsr},
journal = {Quarterly Progress and Status Report},
title = {{The source filter concept in voice production}},
year = {1981}
}
@article{Wrench1999,
author = {Wrench, A.},
url = {http://www.speech.kth.se/gpsr},
title = {{The MOCHA-TIMIT articulatory database.}},
year = {1999}
}


@article{Kubichek1993,
abstract = {The author proposes a perceptually motivated modification to the cepstral distance measure (CD) based on the mel frequency scale and critical-band filtering. The new objective parameter is referred to as the mel cepstral distance (MCD). The author measures and compares the performance of the CD and MCD algorithms by applying them to a dataset representing low-bit-rate code-excited linear prediction (CELP)-coded speech with simulated channel conditions. The improvement in correlation with subjective DAM scores indicates that critical band filtering (and frequency warping) allows better modeling of perceived quality},
author = {Kubichek, Robert F},
doi = {10.1109/PACRIM.1993.407206},
file = {:home/booomkin/Downloads/00407206.pdf:pdf},
isbn = {0780309715},
journal = {IEEE Pacific Rim Conference on Communications, Computers and Signal Processing},
pages = {125--128},
title = {{Mel-cepstral Distance Measure for Objective Speech Quality Assessment}},
year = {1993}
}
@article{Gonzalez2017,
abstract = {To help people who have lost their voice following total laryngectomy, we present a speech restoration system that produces audible speech from articulator movement. The speech articulators are monitored by sensing changes in magnetic field caused by movements of small magnets attached to the lips and tongue. Then, articulator movement is mapped to a sequence of speech parameter vectors using a transformation learned from simultaneous recordings of speech and articulatory data. In this work, this transformation is performed using a type of recurrent neural network (RNN) with fixed latency, which is suitable for real-time processing. The system is evaluated on a phonetically-rich database with simultaneous recordings of speech and articulatory data made by non-impaired subjects. Experimental results show that our RNN-based mapping obtains more accurate speech reconstructions (evaluated using objective quality metrics and a listening test) than articulatory-to-acoustic mappings using Gaussian mixture models (GMMs) or deep neural networks (DNNs). Moreover, our fixed-latency RNN architecture provides comparable performance to an utterance-level batch mapping using bidirectional RNNs (BiRNNs).},
author = {Gonzalez, Jose A. and Cheah, Lam A. and Green, Phil D. and Gilbert, James M. and Ell, Stephen R. and Moore, Roger K. and Holdsworth, Ed},
doi = {10.21437/Interspeech.2017-802},
file = {:home/booomkin/Downloads/0802.PDF:PDF},
isbn = {978-1-61782-123-3},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Articulatory-to-acoustic mapping,Recurrent neural network,Speech rehabilitation,Speech synthesis},
pages = {3986--3990},
pmid = {10904154},
title = {{Evaluation of a silent speech interface based on magnetic sensing and deep learning for a phonetically rich vocabulary}},
volume = {2017-Augus},
year = {2017}
}
@article{Rudzicz2012,
abstract = {This paper describes the acquisition of a new database of dysarthric speech in terms of aligned acoustics and articulatory data. This database currently includes data from seven individuals with speech impediments caused by cerebral palsy or amyotrophic lateral sclerosis and age- and gender-matched control subjects. Each of the individuals with speech impediments are given standardized assessments of speech-motor function by a speech-language pathologist. Acoustic data is obtained by one head-mounted and one directional microphone. Articulatory data is obtained by electromagnetic articulography, which allows the measurement of the tongue and other articulators during speech, and by 3D reconstruction from binocular video sequences. The stimuli are obtained from a variety of sources including the TIMIT database, lists of identified phonetic contrasts, and assessments of speech intelligibility. This paper also includes some analysis as to how dysarthric speech differs from non-dysarthric speech according to features such as length of phonemes, and pronunciation errors.},
author = {Rudzicz, Frank and Namasivayam, Aravind Kumar and Wolff, Talya},
doi = {10.1007/s10579-011-9145-0},
issn = {1574020X},
journal = {Language Resources and Evaluation},
keywords = {Articulation,Dysarthria,Speech},
title = {{The TORGO database of acoustic and articulatory speech from speakers with dysarthria}},
year = {2012}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {1308.0850},
author = {Graves, Alex},
eprint = {1308.0850},
file = {:home/booomkin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves - 2013 - Generating Sequences With Recurrent Neural Networks.pdf:pdf},
month = {aug},
title = {{Generating Sequences With Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo-ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con-vergence properties of the algorithm and provide a regret bound on the conver-gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v9},
author = {Kingma, Diederik P and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v9},
isbn = {8497561635},
journal = {ICLR: International Conference on Learning Representations},
title = {{Adam: A method for stochastic gradient descent}},
year = {2015}
}
@article{Chollet2015,
abstract = {Deep Learning library for Python. Convnets, recurrent neural networks, and more. Runs on Theano or TensorFlow.},
author = {Chollet, Francois},
doi = {10.1111/j.1439-0310.1985.tb00118.x},
isbn = {1439-0310},
issn = {00443573},
journal = {GitHub Repository},
title = {{Keras: Deep Learning library for Theano and TensorFlow}},
year = {2015}
}
@article{Kawahara2006,
abstract = {STRAIGHT, a speech analysis, modification synthesis system, is an extension of the classical channel VOCODER that exploits the advantages of progress in information processing technologies and a new conceptualization of the role of repetitive structures in speech sounds. This review outlines historical backgrounds, architecture, underlying principles, and representative applications of STRAIGHT.},
author = {Kawahara, Hideki},
doi = {10.1250/ast.27.349},
issn = {1346-3969},
journal = {Acoustical Science and Technology},
title = {{STRAIGHT, exploitation of the other aspect of VOCODER: Perceptually isomorphic decomposition of speech sounds}},
year = {2006}
}
