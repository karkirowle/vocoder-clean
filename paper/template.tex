\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2018}
\usepackage{tikz}
\usepackage{url}
\usepackage{blindtext}
\usetikzlibrary{shapes,shadows,arrows,positioning}

\tikzstyle{block} = [draw, fill=white, rectangle, 
    minimum height=3em, minimum width=6em, rounded corners, drop shadow]
\tikzstyle{sum} = [draw, fill=white, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

\title{Towards pathological speech synthesis from articulation}
\name{Bence Halpern$^1$$^2$, Rob J. J. H. van Son$^1$, Michiel W. M.
  van den Brekel$^1$$^2$}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  $^1$NKI-AVL, Amsterdam\\
  $^2$ACLC, University of Amsterdam, The Netherlands}
\email{b.halpern@nki.nl, r.v.son@nki.nl}

\begin{document}

\maketitle
% 
\begin{abstract}

  This paper presents a technique to synthetise speech that is pathological on the articulation level.
  The technique presented considers a vocoder combined with a speaker-independent articulatory to acoustic neural network using electromagnetic articulography recordings from the three largest articulatory datasets. A speech language pathologist indicated that the synthetised samples reflect some qualities of disordered speech. A visualisation technique is also described to shed light on what these neural networks learn.
\end{abstract}
\noindent\textbf{Index Terms}: computational paralinguistics, articulatory-to-acoustic
speech synthesis, deep learning, pathological speech

\section{Introduction}
Understanding how articulation affects speech is a central question in speech
research. The source-filter model was one of the first models to tackle this
problem by discovering that speech production could be described by
the geometry of the vocal tract and the glottal wave. Mathematically,
the source-filter model synthetises speech by exciting an autoregressive (AR) model with a
signal, where the AR coefficients capture the geometry of the
vocal tract, which is also represented by an area function \cite{Benesty2009} \cite{Fant1981}.
Recently, deep learning methods became popular
to understand articulation. These methods use a measurement tool,
called electromagnetic articulography (EMA) to obtain articulation data 
\cite{Aryal2016} \cite{Taguchi} \cite{Liu2018} along with recurrent
neural networks, which are neural networks that are able to deal with
the sequential nature of data \cite{Hochreiter1997}. Data-driven methods became of interest also
in real-time speech synthesis. The efficacy of real-time speech synthesis
has been investigated using a technique called permanent magnetic articulography
by \cite{Gonzalez2017}, which gave intelligeble
speech.

The conclusion of these endeavours were that while it is possible to
predict some of the pitch from articulation, the quality suffers.
However, it is possible to obtain satisfactory values for the
cepstrum.

This indicates, that this technique could be a good candidate for synthetising
pathological speech where the pitch of the voice is natural. For example,
in the case of tongue cancer speech, the laryngeal function remains intact,
meaning the pitch remains natural. Thus, it is proposed that the \( F_0 \) could
be simply obtained through vocoder analysis and only predict the cepstral
values through articulation to model pathologies.

Pathological speech synthesis could have many potential applications, for example, improving
pathological speech detection using synthetic data. It could be also used as a clinical tool,
in patient counselling to prepare them for post-operative speech quality. A deeper understanding
of the articulatory to speech relationship could also offer improvements in speech therapy tools.

In this paper, a technique is described which combines healthy speech
from the three largest articulatory datasets, MNGU0 \cite{Richmond2011},
MOCHA-TIMIT and TORGO \cite{Rudzicz2012} for the first time, in order to
create a general speaker-independent articulatory to acoustic model.

\vspace{0.5em}
The main contributions of this paper are,
\begin{itemize}
\setlength\itemsep{-0.3em}
\item a description of a method for speaker-independent MFCC prediction in Section \ref{section:method}
\item a technique to incorporate articulation domain-knowledge into pathological
  speech synthesis in Section \ref{section:speech} 
\item a discussion of the current limitations of this framework in Section \ref{section:limitations}
\item an attempt to shed light on what these neural networks might learn in Section \ref{section:visualisation}
\end{itemize}

The research code is also available as a Github repository online. \cite{Halpern2019}

\section{Method} \label{section:method}
\subsection{Dataset preprocessing}

Preprocessing technique of previous publications are summarised in Table \ref{tab:example}.
\subsubsection{Electrode preprocessing}

Articulators recorded are slightly different, meaning particular attention has to be paid to align these datasets. An example of EMA recording locations are shown on Figure \ref{fig:electrodes}. Seven electrodes were used for this experiment out of the total eight, Table \ref{tab:electrodes} includes
the alignment of the channels that were used. This ensures that each
input channel records reasonably similar information, meaning that the
channels should have similar variance. These are then standardised on a per speaker
basis. These alleviate some
of the speaker-wise deviations, but does not alleviate problems if an electrode
falls off during the experiment or if an electrode needs to be changed.

In the case of the TORGO dataset, some of the channels contained artifacts,
these have been excluded. The signal to noise
ratio in these spiky regions was low enough to affect
training.

Previously \cite{Gonzalez2016}, the effect of delay on the
output signal were investigated. It has been found that delay
is beneficial for the case of causal models. In Section \ref{section:nnexperiment},
a bidirectional recurrent model will be introduced which is not causal, meaning there is no need for delays.

\begin{table}[th]
  \caption{Articulatory information recorded in datasets}
  \label{tab:electrodes}
  \centering
  \begin{tabular}{ r r r  }
    \toprule
    \textbf{MNGU0} & \textbf{MOCHA-TIMIT} & \textbf{TORGO} \\ 
    \midrule
    Tongue dorsum (T3) & Tongue dorsum (T3) & Tongue back \\
    Tongue blades (T2) & Tongue blades (T2) & Tongue middle \\
    Tongue tip (T1) & Tongue tip (T1) & Tongue tip \\
    Lower incisor (T3) & Jaw & Lower incisor\\
    Upper incisor & Nose & Upper incisor\\
    Upper lip & Upper lip & Upper lip \\
    Lower lip & Lower lip  & Lower lip\\
    \bottomrule
    \end{tabular}
\end{table}


\begin{figure}[t]
  \caption{The visualisation of electrode locations for 300 samples from
    the MNGU0 dataset at their initial position.}
  \label{fig:electrodes}
  \begin{center}
    \scalebox{0.50}{\input{init_pos.pgf}}
\end{center}
\end{figure}

\begin{figure}[t]
  \caption{Thresholded difference mask of activations indicates that a boundary phenomena is learned by the neural network. Best viewed in zoom.}
  \label{fig:mask}
  \begin{center}
    \scalebox{0.40}{\input{blstm_act.pgf}}
\end{center}
\end{figure}
\subsubsection{Speech data processing}


The total dataset contains speech from six British male and three British
female speakers, with a total of 6117 utterances, approximately 10 hours of recorded
speech. The recordings from the microphones all have a sampling frequency of 16kHz.
Only the healthy speech has been included from the TORGO dataset. There are 1263 utterances from the
MNGU0, 920 from the MOCHA-TIMIT and 3934 from the TORGO dataset.

Vocoder features were extracted with the PyWORLD vocoder \cite{Morise2016}
and compressed with the PySPTK toolkit \cite{pysptk}. The period between consecutive
frames were 5 miliseconds. The resulting 40 MFCC and 1 power parameters
were used to generate static and delta parameters, resulting in 82
parameters for the training. As the first step of the MFCC extraction \( \alpha
= 0.42 \) were used as a pre-emphasis coefficient. The PyWORLD vocoder
also provides the $ F_0 $ and BAP values, which were not used for training.

\subsubsection{Sampling}

The sampling frequency of the original EMA signals was 500 Hz, however
the MNGU0 was provided to us downsampled to 200 Hz. To match this frequency,
the sampling frequency of the other datasets was also downsampled to 200 Hz.

For the MNGU0 dataset, NaN (not a number) values occurred when the measurement precision
was low. These values were interpolated linearly. 

To ease training, the input signals were either truncated or padded
so there were a total of \( T = 1000 \) samples for each training example.
For input signals which are shorter, it is assumed that the last part is
silence, so it is padded with the last element. These are not propagated back during training, to avoid the neural network making inference based on the length of the last element.

\subsubsection{Fundamental frequency interpolation}

In this framework, the \( F_0 \) is also used for prediction, thus it
needs to be processed to be used by the neural network. 
Previously, it has been found beneficial to take the logarithm of the
pitch to obtain a continous \( F_0 \) curve in the prediction setting.
When the logarithm is not defined, linear  interpolatation has been done. \cite{Gonzalez2017}
An alternative method with exponential interpolation is described in \cite{Chen1997}.

\subsection{Synthesis setup}

The setup for inference and training can be seen in Figure \ref{fig:structure}.
In the training setup, only the MFCCs are given.
The pitch and band aperiodicities are directly fed to the vocoder
during synthesis time, as these don't contain information about articulation. 

\begin{figure}
  \caption{Red dashed line indicates training-only setup, and blue thick lines indicate inference for speech synthesis. Best viewed in colour.}
  \label{fig:structure}
  \begin{tikzpicture}[auto, node distance=3cm,>=latex']
    \node [input, name=speech] {};
    
    \node [block, right of=speech] (analysis) {Analysis};
    \node [block, below of=analysis] (model) {Model};
    \node [block, right of=model] (synthesis) {Synthesis};
    \node [input, name=ema, left of=model] {};
    \node [right of=analysis, name=bela] {};
    \draw [->] (speech) -- node {$\text{Speech}$} (analysis);
    \draw [->,color=blue,thick] (model.350) -- node {$\text{MFCC}$} (synthesis.190);
    \draw[->,color=red,dashed] (analysis.340) -- +(2em, 0) node[above] {$\text{MFCC}$} |- (model.380);
    \draw[->] (analysis.270) -- node {$F_0$} (model.90);
    \draw[->,color=blue,thick] (analysis.380) -- +(2em,0) node[above] {$\text{BAP}$} -| (synthesis.130);
    \draw[->,color=blue,thick] (synthesis.0) -- +(3em,0) node[above] {$\text{Speech}$};
    \draw[->] (ema) -- node[above] {$\text{EMA}$} (model);
    \draw[->,color=blue,thick] (analysis.90) -- +(0,2em) node[left] {$F_0$} -| (synthesis.50);
    \end{tikzpicture}
\end{figure}

\begin{table}[th]
  \caption{Comparison of preprocessing techniques}
  \label{tab:example}
  \centering
  \footnotesize
  \begin{tabular}{ r r r r }
    \toprule
    \textbf{Author} & \textbf{Liu} & \textbf{Taguchi} & \textbf{Gonzalez} \\
    \midrule
    \textbf{EMA/PMA} & EMA & EMA & PMA \\
    \textbf{MFCC} & 40 + 1 & 40 + 1 & 24 + 1 \\
    \textbf{Delta} & No & Yes & Yes \\
    \textbf{EMA sampling} & 200 Hz & 200 Hz & 100 Hz* \\
    \textbf{Standardisation} & Yes & Yes & Yes \\
    \textbf{Smoothing} & No & Yes  & No \\
    \textbf{Vocoder} & STRAIGHT \cite{Kawahara2006} & WORLD  & STRAIGHT \\
    \bottomrule
  \end{tabular}
  *Upsampled to 200 Hz to match analysis rate
\end{table}

\subsection{Neural network design} \label{section:nnexperiment}

\subsubsection{An empirical look at previous architectures}

In this paper, a recurrent neural network will be used in order to
approximate the articulatory to acoustic mapping. To construct
this speaker-independent network, previous speaker-dependent architectures
have been studied, to conclude on an appropriate design.

The most pressing problem of recurrent neural networks were the issue
of vanishing/exploding gradients. This were somewhat alleviated by the
introduction of Long-Short Term Memory (LSTM) and Gated Recurrent Units (GRU).
This is the reason why \cite{Taguchi} used incremental
training, and probably the reason why \cite{Liu2018} used a learning rate
scheduler. However, \cite{Gonzalez2017} used Adam optimiser \cite{Kingma2015}
which is known to manage both of these problems with the minor disadvantage of
the abscence of good convergence guarantees. The fact that Adam was able
to obtain similar results without careful parameter-tuning indicated that
it will be an appropriate candidate as an optimiser. 

Previous publications on speaker-dependent models reported best
performance on bidirectional architectures, however it was unclear whether
BLSTM or BGRU architectures are better. Also, \cite{Taguchi} resorted to
a combination of fully connected and recurrent layers. One fully connected layer
was also included in all architectures, which effectively performs linear
regression in the end.

In order to determine the best architecture, a pilot study has been
performed on all three neural networks which are summarised in Table \ref{tab:architectures},
however all of them were trained with an Adam optimiser, and a learning rate
of \( 0.003 \), and a batch size of 100 without noise on MNGU0 dataset.
The best performing neural network was then trained on the entire dataset. 


\begin{table}[th]
  \caption{Performance of speaker-independent articulatory to acoustic neural network for 10-fold cross-validation with 95 \% confidence intervals.}
  \label{tab:all_data}
  \centering
  \footnotesize

  \begin{tabular}{ r r r }
    \toprule
    \textbf{Dataset} & Multi-speaker & Single-speaker \\
    & MCD & MCD \\
    \midrule 
    \textbf{Combined result} & 5.31 $\pm$ 0.09 dB & N/A \\  
    \midrule
    \textbf{MNGU0} & 5.93 $\pm$ 0.31 dB & 4.77 dB \\
    \textbf{Female MOCHA-TIMIT} & 5.02 $\pm$ 0.06 dB  & 5.23 dB \\
    \textbf{Male MOCHA-TIMIT} & \textbf{4.06} $\pm$ 0.06 dB & 5.83 dB \\
    \textbf{TORGO Part 1} & 4.48 $\pm$ 0.03 dB & N/A \\
    \textbf{TORGO Part 2} & 4.23 $\pm$ 0.06 dB & N/A \\
    \textbf{TORGO Part 3} & 4.81 $\pm$ 0.14 dB & N/A \\
    \textbf{TORGO Part 4} & 4.94 $\pm$ 0.09 dB & N/A \\
    \textbf{TORGO Part 5} & 4.64 $\pm$ 0.04 dB & N/A \\
    \textbf{TORGO Part 6} & 4.70 $\pm$ 0.05 dB & N/A \\
    \textbf{TORGO Part 7} & 4.62 $\pm$ 0.04 dB & N/A \\
    \textbf{TORGO Part 8} & 15 $\pm$ 0.86 dB & N/A\\
    \textbf{TORGO Part 9} & 4.63 $\pm$ 0.11 dB & N/A \\
    \textbf{TORGO Part 10} & 4.85 $\pm$ 0.12 dB & N/A \\
    \bottomrule
  \end{tabular}
\end{table}


%\begin{table}[th]
%  \caption{Transfer learning comparison with single speaker models}
%  \label{tab:transfer}
%  \centering
%  \footnotesize
%
%  \begin{tabular}{ r r r}
%    \toprule
%    \textbf{Dataset} & \textbf{Speaker only} & \textbf{Transfer preproc} \\
%    \midrule
%    \textbf{MNGU0} & 4.77 dB & N/A \\
%    \textbf{Female MOCHA-TIMIT} & 5.23 dB & 11.43 dB \\
%    \textbf{Male MOCHA-TIMIT} & 5.88 dB & 7.86 dB \\
%    \bottomrule
%  \end{tabular}
%\end{table}

\begin{table}[th]
  \caption{Comparison of different training methods used in previous publications with the
    results of the pilot study using held-out validation. The method described in the paper of
  Gonzalez performed best.}
  \label{tab:architectures}
  \centering
  \footnotesize

  \begin{tabular}{ r r r r }
    \toprule
    \textbf{Author} & \textbf{Liu} & \textbf{Taguchi} & \textbf{Gonzalez} \\
    \midrule
    \textbf{BLSTM layers} & 4 (128) & 2 (256) & 4 (150) GRU \\
    \textbf{Dense layers} & 1 & 3+1 & 1 \\
    \textbf{Regularisation} & No & LayerNorm & Noise 0.05 \\
    \textbf{Dropout} & No & Yes (50 \%) & No \\
    \textbf{Optimiser} & SGD & Grave's RMSProp & Adam \\
    \textbf{Learning rate} & 0.01* & 0.01 & 0.003 \\
    \textbf{Gradient clipping} & No & 5 & No \\
    \textbf{Early stopping} & Yes & Yes & Yes \\
    \textbf{MLPG} \cite{Wu2016} & No & Yes & Yes \\ 
    \textbf{Maximum epochs} & 32 & N/A & 100 \\
    \textbf{Batch size} & N/A & 8 & 100 \\
    \textbf{Incremental training} & No & Yes & Yes \\
    \textbf{MCD***} & 4.84 dB & 7.28 dB & 4.77 dB \\
    \bottomrule
  \end{tabular}
  * with decay after Epoch 11 \\
  ** from author communication \\
  *** results of our training with Adam optimiser
\end{table}

For training the mean squared error loss function was used, and for
evaluation the Mel cepstral distortion (MCD) have been employed. \cite{Kubichek1993}

For the speaker-independent experiments, ten fold cross-validation was performed
to estimate the out-ouf-sample generalisation capability of the neural networks.

\subsection{Articulatory space modification} \label{section:speech}

Using this framework, the problem of making pathological speech
can be traded for the problem of making pathological articulation and
feeding pathological articulation through the neural network.

In order to make the articulation pathological, two methods were employed.
The first method fixes the position of the tongue tip in the initial position.
The second method decreases the velocity of the tongue via thresholding discrete time differences,
and reconstructing the position with performing a cumulative sum.




\section{Results and discussion}

\subsection{Pilot study}

The pilot study results are summarised in Table \ref{tab:pilot}.

Based on our training, it seems clear that the GRU architecture was superior to an LSTM
architecture in our case, when used with an Adam optimiser.
There is no general consesus whether GRU or LSTM is better for particular
datasets. \cite{Jozefowicz2015}

\subsection{Prediction of MFCC values}

The prediction results for the MFCC values are summarised in Table
\ref{tab:all_data}. Results were all in similar range as previously
reported values for speaker-dependent datasets, and in our framework
the speaker-independent architectures clearly performed better than the
speaker-dependent architectures on the MOCHA-TIMIT datasets.

\subsection{Learning curves (TODO)}

The training set was increased from ten percent of it's total size to it's
total size in increments of ten percents, and the mean squared error
was calculated at all epochs of training for the validation set, which
can be seen on Figure \ref{learning_curve}.

\begin{figure}[t]
  \begin{center}
    \scalebox{0.50}{\input{retraining.pgf}}
    \caption{Partial data retraining shows that adding more data would
      decrease loss}
      \label{learning_curve}
\end{center}
\end{figure}


After that, a paired t-test was performed to answer whether there is
a statistically significant improvement with each addition of the training data.
It has been found that for each addition, the paired t-test resulted in statistically
significant results, which indicates that it is very likely that addition
of more data improves the model.

To estimate how much data would be needed to achieve a ''perfect'' performance,
assuming a linear fit, the mean of the validation loss in last 5 epochs
were taken and regressed againts the amount of training data included in
number of samples. This fit can be seen on Figure \ref{retraining_linear}.
Taking the ratio of the slope and the intercept, approximately 8382
training data point would be needed.

Again, note that there are several limitations of this assumption. First, the
relationship is most likely not linear. Secondly, given any noise, achieving
zero loss is impossible. However, these benchmarks still have merit in
future experiment design.

\subsection{What do these neural networks learn?} \label{section:visualisation}


Recently, there have been many advancements in understanding what neural networks learn.
Convolutional neural networks can be analysed via conventional methods in filter analysis,
classification neural networks can propagate back gradients to find the most important inputs for
the prediction. These techniques are not applicable for recurrent neural networks in a regression
context, so we resort to exploring the activations outputs of the layers.

To make these intelligible, a difference mask is thresholded to find oriented peaks in the spectra.
On Figure \ref{fig:mask}, two things can be observed. Firstly, line-like boundaries are learned, and their duration indicates these might approximate phone to word level representations. Secondly, as the activations propagate through the deeper layers of the neural networks, these line like boundaries are better approximated.

\subsection{The current limitations of the synthesis} \label{section:limitations}
According to our observations, the quality is bounded more by the
quality of the vocoder, than the synthesis itself. In terms of mean square
error (MSE), it has been found that difference between the vocoder resynthesised speech
and the predicted speech has an MSE of 11. The MSE
between analysis-resynthesis and the vocoder is 80. That means that 88\% of the performance loss
is due to the vocoder analysis-resynthesis. This indicates that
future improvements should focus on better vocoding rather than better
acoustic mapping.


\subsection{Pathological speech examples}

Some synthetised pathological speech examples can be found on the webpage of the
author, see \cite{Speech2019}.
Informal discussions with speech language pathologist indeed confirmed
that some of these synthetised samples resemble dysarthic or disordered speech,
but these simple heuristics usually don't incorporate enough knowledge
about a particulator pathology to show it consistently.

This confirms that that the framework can be used as platform to implement a
pathological articulation by reflecting the physiological changes in the articulatory
space. This is most easily done in a data-driven fashion, by recording healthy
and pathological articulation and creating a pathology-dependent mapping
in the articulatory space. Because it is a low dimensional space, this
is a much easier problem than learning the same mapping in the cepstral

\section{Conclusion}

This paper is a proof of concept that it is possible to make pathological
speech by incorporating changes in an articulatory domain. Benchmarks
have been also established and an open source repository is also available
in order to reproduce these results. 

It can be concluded that it is possible to make speech that resembles
pathological speech from impaired articulation. Further work needs to be done on improving
speech quality and creating models which consistently show a certain
pathology.

\section{Acknowledgements}
This project has received funding from the European Union's Horizon
2020 research and innovation programme under Marie Sklodowska-Curie
grant agreement No 766287.
This work was carried out on the Dutch national e-infrastructure with the support of SURF Cooperative.
The Department of Head and Neck Oncology and surgery of the Netherlands
Cancer Institute receives a research grant from Atos Medical (Malmö,
Sweden), which contributes to the existing infrastructure for quality of
life research.

\bibliographystyle{IEEEtran}

\bibliography{paper1}


\end{document}
