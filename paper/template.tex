\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2018}
\usepackage{tikz}
\usepackage{url}
\usepackage{blindtext}
\usetikzlibrary{shapes,arrows,positioning}

\tikzstyle{block} = [draw, fill=white, rectangle, 
    minimum height=3em, minimum width=6em]
\tikzstyle{sum} = [draw, fill=white, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

\title{Multi-speaker articulatory-to-acoustic speech synthesis with
deep neural networks}
\name{Bence Halpern$^1$$^2$, Rob J. J. H. van Son$^1$, Michiel W. M.
  van den Brekel$^1$$^2$}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  $^1$NKI-AVL, Amsterdam\\
  $^2$ACLC,University of Amsterdam, The Netherlands}
\email{b.halpern@nki.nl, r.v.son@nki.nl}

\begin{document}

\maketitle
% 
\begin{abstract}
  \blindtext[1]
\end{abstract}
\noindent\textbf{Index Terms}: computational paralinguistics, articulatory-to-acoustic
speech synthesis, deep learning, pathological speech

\section{Introduction}

Understading how articulation affects speech is a central question in speech
research. The source-filter model was one of the first models to tackle this
problem by noting that speech production could be described by
the geometry of the vocal tract and the glottal wave. Mathematically,
the source-filter model synthetises speech by exciting an autoregressive (AR) model with a
signal, where the AR coefficients capture the geometry of the
vocal tract, which is also represented by an area function \cite{Benesty2009}. 

\begin{figure}[t]
  \begin{center}
    \scalebox{0.75}{
  \begin{tikzpicture}[auto, node distance=5cm,>=latex']
    \node [input] (f0) {$F_0$};
  \node [block, below right = 1 cm and 2 cm of f0] (predictor) { \textbf{Neural network} $f(\mathbf{x}[t], \mathbf{F_0}[t]) $ };
  \node [input, name=input, right of = predictor] (speech) {Speech};
    \node [input, below left = 1 cm and 2 cm of predictor] (ema) {};
    \draw [->] (f0) -- node {$ F_0[t] $}   (predictor.170);
    \draw [->] (ema) -- node {$ \text{EMA} = \mathbf{x}[t] $}   (predictor.190);
    \draw [->] (predictor) -- node {$ \text{Speech} = \mathbf{y}[t] $} (speech);
  \end{tikzpicture}
}

    \scalebox{0.75}{
  \begin{tikzpicture}[auto, node distance=5cm,>=latex']
    \node [input] (f0) {$F_0$};
  \node [block, right of = f0] (predictor) { \textbf{Vocal tract} $f(\mathbf{x}[t]) $ };
  \node [input, name=input, right of = predictor] (speech) {Speech};
    \draw [->] (f0) -- node {$ F_0[t] $}   (predictor);
    \draw [->] (predictor) -- node {$ \text{Speech} = \mathbf{y}[t] $} (speech);
  \end{tikzpicture}
 }
 \caption{Black box diagram showing the parallelities of a neural network
   based model with a source-filter model}
  \label{fig1}
    \end{center}
  \end{figure}

Recently, deep learning methods became popular
to understand articulation. These methods use a measurement tool,
called electromagnetic articulography to obtain articulation data 
\cite{Aryal2016} \cite{Taguchi} \cite{Liu2018} along with recurrent
neural networks, which are neural networks that are able to deal with
the sequential nature of data. Data-driven methods became of interest also
in real-time speech synthesis. The efficacy of real-time speech synthesis
has been investigated using a tool called permanent magnetic articulography
by Gonzalez et al \cite{Gonzalez2017}, which gave understandable
speech.

The conclusion of these endeavours were that while it is possible to
predict some of the pitch from articulation, the quality suffers.
It is, however, possible to obtain satisfactory values for the
cepstral quality.



In the author's broader research, the aim is to demonstrate how pathologies in
articulation are translated to speech. In some types of pathological
speech the laryngeal function remains intact, meaning the \( F_0 \)
remains unchanged. This means, by knowing the original pitch and the
pathological deviations in articulation, it should be possible to
synthetise pathological speech.

The idea with MFCCs is to separate vocal tract information from the excitation
information. Thus based on the electrode signals of the vocal tract configuration
it should be possible to predict part of the spectra only dependent on the vocal
tract configuration.

The main contributions of this paper are,
\begin{itemize}
\item a set of benchmarks for multi-speaker articulatory to acoustic synthesis
\item estimation on how much data would be needed to construct a better model
\item an example of how to construct pathological speech using this framework
\end{itemize}

Our code is also available as a Github repository on the link
\url{https://github.com/karkirowle/vocoder-clean}.

\section{Method}
\subsection{Electromagnetic articulography}

Electromagnetic articulography (EMA) is a measurement technology which uses
sensor coils which are placed on the different articulators in the vocal tract.
Using this technology it is possible to record the displacement of the articulators
which can then be used to predict speech which is simultaneously recorded.

The electrodes are placed on the MNGU0 \cite{Richmond2011} and MOCHA-TIMIT dataset on a total
of seven positions. The two datasets have not recorded entire the
same channels, so Table \ref{tab:electrodes} include the alignment
that has been used.
\begin{table}[th]
  \label{tab:electrodes}
  \caption{Articulatory information recorded in datasets}
  \centering
  \begin{tabular}{ r r }
    \toprule
    \textbf{MNGU0} & \textbf{MOCHA-TIMIT} \\
    \midrule
    Tongue dorsum (T3) & Tongue dorsum (T3) \\
    Tongue blades (T2) & Tongue blades (T2) \\
    Tongue tip (T1) & Tongue tip (T1) \\
    Lower incisor (T3) & Jaw \\
    Upper incisor & Nose \\
    Upper lip & Upper lip \\
    Lower lip & Lower lip \\
    \bottomrule
    \end{tabular}
\end{table}

TODO: Decide whether you want to leave out the lower incisor or jaw

\begin{figure}[t]
  \begin{center}
    \scalebox{0.50}{\input{init_pos.pgf}}
  \caption{The visualisation of electrode locations for all samples in
    the MNGU0 dataset at time point \( t = 0 \)}
\end{center}
\end{figure}
\subsection{Dataset}

The MNGU0 and the MOCHA-TIMIT dataset were used to obtain a total of
2274 samples, which were partitioned to training and validation set when
no hyperparameter optimisation was done. An additional test set was used
in case of hyperparameter optimisation. The dataset contains data from
three speakers, two British male and one British female. The recordings
from the microphones were 16kHz.

\subsection{Sampling}

It is important that the input and output sample rates of the different
datasets are matched, because otherwise, the input-output pairs contain
different amounts of information.  

The sampling frequency of the original EMA signals were 500 Hz, however
the MNGU0 was provided to us downsampled to 200 Hz. To match this frequency,
the sampling frequency of the MOCHA-TIMIT were also downsampled to 200 Hz.

For the MNGU0 dataset, NaN values occured when the measurement precision
was lose. These values were simply interpolated linearly.

To ease training, the input signals were either truncated or padded
so there were a total of \( T = 1000 \) samples for each training example.
For input signals which are shorter, it is assumed that the last part is
silence, so it is padded with the last element.

\subsection{Vocoder analysis}

Vocoder features were extracted with the PyWORLD vocoder \cite{Morise2016}
and compressed with the PySPTK toolkit available at \url{http://github.com/r9y9/pysptk}.
The period between consecutive frames were 5 miliseconds. The resulting 40 MFCC
and 1 power parameters were used to generatic static and delta parameters,
resulting in 82 parameters for the training. As the first step of the MFCC extraction \( \alpha
= 0.42 \) were used a pre-emphasis coefficient. The PyWORLD vocoder
also provides the $ F_0 $ and BAP values for synthesis, these were
explicitly given for the synthesis, due to the reasons stated in the
Introduction.

\subsection{Delay}
Previously \cite{Gonzalez2016}, the effect of delay on the
output signal were investigated. While it has been found that delay
is beneficial, the author's choice of function approximators are
restricted to acausal models, so it has been decided
not to use delay in our final implementation.

\subsection{Fundamental frequency interpolation}

Previously, it has been found beneficial to take the logarithm of the
pitch to obtain a continous \( F_0 \) curve.
When the logarithm is not defined, linear  interpolatation has been done. \cite{Gonzalez2017}
An alternative method also exists with exponential interpolation
which is described in \cite{Chen1997}. It has been decided that the
linear interpolation technique will be used. 

\subsection{Normalisation}

Exploratory data analysis indicated that the articulatory trajectories of
the datasets were on different scale and bias, so it was normalised
on a per speaker basis. The output MFCCs were normalised for each cepstral
coefficients.

\subsection{Neural network}

To approximate the functional relationship, the authors have decided to
train a Bidirectional Long-Short Term Memory based neural network \cite{Hochreiter1997}. 

For all BLSTM layers, the CuDNNLSTM implementation have been used to
improve efficiency, however this limits the activation function to be
stricly \( \text{tanh}(\cdot) \)

A LSTM is governed by the following equations,

\begin{equation}
  \mathbf{i}_{t} = \sigma ( \mathbf{W}_{xi} \mathbf{x}_{t} + \mathbf{W}_{hi}
  \mathbf{h}_{t-1} + \mathbf{W}_{ci} \mathbf{c}_{t-1} + \mathbf{b}_{i}),  
\end{equation}
\begin{equation}
  \mathbf{f}_{t} = \sigma ( \mathbf{W}_{xf} \mathbf{x}_{t} + \mathbf{W}_{hf}
  \mathbf{h}_{t-1} + \mathbf{W}_{cf} \mathbf{c}_{t-1} + \mathbf{b}_{f}), 
\end{equation}
\begin{equation}
  \mathbf{c}_{t} = \mathbf{f} \mathbf{c}_{t-1} + \mathbf{i}_{t} \text{tanh} ( \mathbf{W}_{hc}
  \mathbf{h}_{t-1} + \mathbf{b}_c),
\end{equation}
\begin{equation}
  \mathbf{o}_{t} = \sigma ( \mathbf{W}_{x0}\mathbf{x}_t + \mathbf{W}_{ho} \mathbf{h}_{t-1}
  + \mathbf{W}_{co}\mathbf{c}_{t} + \mathbf{b}_0 ),
\end{equation}
\begin{equation}
  \mathbf{h}_t = \mathbf{o}_{t} \text{tanh} ( \mathbf{c}_t ).
  \end{equation}

The exact architectures are described in the Section \ref{nnexperiment}.

\subsection{Synthesis}

Synthesis for the validation set is performed using the ground truth
parameters for the BAP and $ F_0 $ and only the spectral prediction is
benchmarked.

\begin{table}[th]
  \caption{Comparison of preprocessing techniques}
  \label{tab:example}
  \centering
  \footnotesize
  \begin{tabular}{ r r r r }
    \toprule
    \textbf{Author} & \textbf{Liu} & \textbf{Taguchi} & \textbf{Gonzalez} \\
    \midrule
    \textbf{EMA/PMA} & EMA & EMA & PMA \\
    \textbf{MFCC} & 40 + 1 & 40 + 1 & 24 + 1 \\
    \textbf{Delta} & No & Yes & Yes \\
    \textbf{EMA sampling} & 200 Hz & 200 Hz & 100 Hz* \\
    \textbf{Input standardisation} & Yes & N/A  & Yes \\
    \textbf{Trajectory smoothing} & No & Yes  & No \\
    \textbf{Output standardisation} & Yes & Yes & Yes \\
    \textbf{Vocoder} & STRAIGHT & WORLD  & STRAIGHT \\
    \bottomrule
    
  \end{tabular}
  *Upsampled to 200 to match analysis rate
\end{table}

\section{Experiment}

\subsection{Neural network experiments} \label{nnexperiment}

Three recurrent neural networks architectures were trained based on previous
papers tackling similiar problems.

Based on \cite{Liu2018}, a bidirectional LSTM was trained with four layers.
The final layer was a fully connected layer matching the output dimensions.
This neural network was trained using stochastic gradient descent and
a learning rate of \( \alpha = 0.01 \).

Another publication \cite{Taguchi} used a neural networks with three fully
connected layers with 128 hidden units, each layer has a linear activation
function, which is followed by Layer Normalisation and a sigmoid activation
function. This is followed by two BLSTM layers with 256 hidden units. Finally,
a fully connected layer is used. This neural network was trained using Grave's RMSProp (TODO: ref),
with a learning rate of \( \alpha = 0.01 \) .

Finally, another BLSTM was trained based on the architecture of
\cite{Liu2018}, but using Adam as an optimiser \cite{Kingma2015}.

Determining a good set of architecture and parameter settings is the most difficult part of
the experiment design. Bidirectional LSTMs have performed the best on predicting the MFCC spectra in
all related papers. Taguchi uses fully-connected layers and while it not justified in the paper, it
is reasonable to assume that this does pre-processing of the time-series. 

It can be easily seen mathematically that after after the last LSTM layer, a fully connected layer is also needed as this acts as a linear regression based on the LSTM parameters. This is because the range of the activation function is not sufficient to capture the range of the normalised MFCCs. It is possible to introduce some structural bias by using the same linear regression from each frame, somewhat limiting complexity. 

In terms of the optimisation, Adam and Graves's RMSProp is preferred, which is not surpsing due to the
fact that these known to be better for optimising non-stationary objectives. With the SGD,
learning rate scheduling had to be used. 

In order to choose the best architecture, all of them have been reimplemented with slight modifications.
This is a better practice, because the MCD's were interpreted differently on the different papers, i.e the error in the silence frames were not taken into consideration. All of them were compared on the mngu0 dataset. The best performing architecture have been retrained on the full dataset, using the preprocessing techniques mentioned above, and an MCD of 5.48 have been obtained.


A possible approach is to use transfer learning to get better performance.

The idea is either to freeze the initial model trained on the mngu0, then use pre-processing
layers to learn the feature mappings. 
Some valid questions: generalisation by validating on certaind datasets.

\begin{table}[th]
  \caption{Transfer learning comparison with single speaker models}
  \label{tab:transfer}
  \centering
  \footnotesize

  \begin{tabular}{ r r r}
    \toprule
    \textbf{Dataset} & \textbf{Speaker only} & \textbf{Transfer} \\
    \midrule
    \textbf{MNGU0} & 4.77 dB & N/A \\
    \textbf{Female MOCHA-TIMIT} & 5.23 dB & S \\
    \textbf{Male MOCHA-TIMIT} & 5.88 dB & S \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[th]
  \caption{Held out validation for different architectures on MNGU0}
  \label{tab:example}
  \centering
  \footnotesize

  \begin{tabular}{ r r r r }
    \toprule
    \textbf{Author} & \textbf{MCD} \\
    \midrule
    \textbf{Gonzalez} & 4.77 dB \\
    \textbf{Taguchi} & 7.28 dB \\
    \textbf{Liu} & 4.84 dB \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[th]
  \caption{Comparison of different trainng methods}
  \label{tab:example}
  \centering
  \footnotesize

  \begin{tabular}{ r r r r }
    \toprule
    \textbf{Author} & \textbf{Liu} & \textbf{Taguchi} & \textbf{Gonzalez} \\
    \midrule
    \textbf{BLSTM layers} & 4 (128) & 2 (256) & 4 (150) GRU \\
    \textbf{Dense layers} & 1 & 3+1 & 1 \\
    \textbf{Regularisation} & No & LayerNorm & Noise 0.05 \\
    \textbf{Dropout} & No & Yes (50 \%) & No \\
    \textbf{Optimiser} & SGD & Grave's RMSProp & Adam \\
    \textbf{Learning rate} & 0.01* & 0.01 & 0.003 \\
    \textbf{Gradient clipping} & No & 5 & No \\
    \textbf{Early stopping} & Yes & Yes & Yes \\
    \textbf{MLPG} & No & Yes & Yes \\ 
    \textbf{Maximum epochs} & 32 & N/A & 100 \\
    \textbf{Batch size} & N/A & 8 & 100 \\
    \textbf{Incremental training} & No & Yes & Yes \\
    \bottomrule
  \end{tabular}
  * with decay after Epoch 11 \\
  ** from author communication
\end{table}
\begin{table}[th]
  \caption{Comparison of different trainng methods}
  \label{tab:example}
  \centering
  \begin{tabular}{ r r r }
    \toprule
    \textbf{Architecture} & \textbf{Optimiser} & \textbf{Base learning rate} \\
    \midrule
    Liu 2018 & SGD & 0.01 \\
    Taguchi 2018 & RMSProp & 0.01       \\
    Modified Liu & Adam & 0.01               \\
    \bottomrule
  \end{tabular}
  
\end{table}

\begin{figure}[t]
  \begin{center}
    \scalebox{0.75}{\input{blstm_act.pgf}}
    \caption{Something}
\end{center}
\end{figure}
iii
For training the mean squared error loss function was used, and for
evaluation the Mel cepstral distortion (MCD) have been employed. \cite{Kubichek1993}

Ten fold cross-validation was performed to estimate the out-ouf-sample
generalisation capability of the neural networks.

\subsection{Pathological speech synthesis}

Pathological speech synthesis is performed by considering the articulatory
space and taking knowledge about the change of articulation.

In tongue cancer, articulation of the tongue is impeded. In practice,
it is found that teaching patients to speak at a slower rate helps these
articulation problems. Thus, it is hypothesised that is the maximum
velocity of the tongue that is limited in pathological speech.

A pathological speech transformation could then be constructed for
a discretet time signal by first taking the discrete time difference,

\begin{equation}
  d[t] = x[t] - x[t-1],
  \label{eq1}
\end{equation}

where \( x[t] \in \mathbb{R}^T \) is a signal for one particular electrode channel.

The difference signal then can be thresholded using,

\begin{equation}
  d_p[t] = \text{min}(d[t],c) \quad \text{for} \quad d[t] \geq 0, 
  \label{eq2}
\end{equation}
\begin{equation}
  d_p[t] = \text{min}(d[t],-c) \quad \text{for} \quad d[t] < 0,
  \label{eq3}
\end{equation}

where \( c \in \mathbb{R}^{+} \) is a positive number representing an
arbitrary threshold.

After obtaining this signal a cumulative sum could be performed to obtain
the pathological EMA signal
\begin{equation}
  p[t] = \sum_{i=0}^{t} x[i].
  \label{eq4}
\end{equation}

This signal then could be fed through a feedforward run of a neural network
to synthetise pathological speech.

\section{Results and discussion}

\subsection{Benchmark results}
\begin{table}[th]
  \caption{Comparison of 10-fold CV performance of neural networks}
  \label{tab:example}
  \centering
  \begin{tabular}{ r r }
    \toprule
    \textbf{Architecture} & \textbf{MCD} \\
    \midrule
    Liu 2018 & 5.8 dB \\
    Taguchi 2018 & 6.2 dB               \\
    Modifed Liu & 6 dB               \\
    \bottomrule
  \end{tabular}
  
\end{table}

It is important to note that the original networks were trained on
single speakers, that is why the MCD values are higher than in the
original publications.


\subsection{Learning curves}

The training set was increased from ten percent of it's total size to it's
total size in increments of ten percents, and the mean squared error
was calculated at all epochs of training for the validation set, which
can be seen on Figure \ref{learning_curve}.

\begin{figure}[t]
  \begin{center}
    \scalebox{0.50}{\input{retraining.pgf}}
    \caption{Partial data retraining shows that adding more data would
      decrease loss}
      \label{learning_curve}
\end{center}
\end{figure}


After that, a paired t-test was performed to answer whether there is
a statistically significant improvement with each addition of the training data.
It has been found that for each addition, the paired t-test resulted in statistically
significant results, which indicates that it is very likely that addition
of more data improves the model.

To estimate how much data would be needed to achieve a ''perfect'' performance,
assuming a linear fit, the mean of the validation loss in last 5 epochs
were taken and regressed againts the amount of training data included in
number of samples. This fit can be seen on Figure \ref{retraining_linear}.
Taking the ratio of the slope and the intercept, approximately 8382
training data point would be needed.

Again, note that there are several limitations of this assumption. First, the
relationship is most likely not linear. Secondly, given any noise, achieving
zero loss is impossible. However, these benchmarks still have merit in
future experiment design.

\begin{figure}[t]
  \begin{center}
    \scalebox{0.50}{\input{retraining_linear.pgf}}
    \caption{Partial data retraining shows that adding more data would
      decrease loss. }
    \label{retraining_linear}
\end{center}
\end{figure}


\subsection{Pathological speech examples}

The pathological speech examples can be listened on the webpage of the
author, see \url{http://karkirowle.github.io/paper1}. 

\section{Conclusion}

This paper is a proof of concept that it is possible to make pathological
speech by incorporating changes in an articulatory domain. Benchmarks
have been also established and an open source repository is also available
in order to reproduce these results. Using cross-validation as the
training data increases, bounds have been established on the expected
amount of data needed for the model to improve.

It can be concluded that it is possible to make satisfactory quality
speech synthesis, and it is possible to present some lisping pathologies.

In future work, changes in articulation during oral cancer will be
investigated using oral cancer to use empirical data for speech synthesis
instead of physical considerations.

\section{Acknowledgements}
This project has received funding from the European Union's Horizon
2020 research and innovation programme under Marie Sklodowska-Curie
grant agreement No 766287.



\bibliographystyle{IEEEtran}

\bibliography{paper1}


\end{document}
